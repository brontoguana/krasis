================================================================
Krasis Benchmark — 2026-02-16 16:51:54
================================================================
Model:            Qwen3-Coder-Next
Architecture:     qwen3_next, 48 layers, 512 experts, top-10, 12 GQA + 36 linear
PP Partition:     [48] (1 GPUs)

Hardware:
  CPU:            AMD EPYC 7742 64-Core Processor (64 cores)
  RAM:            995 GB total, 76.7 GB used by process
  GPU 0:          NVIDIA RTX 2000 Ada Generation (16380 MB), 12272 MB allocated
  GPU 1:          NVIDIA RTX 2000 Ada Generation (16380 MB), 14817 MB allocated
  GPU 2:          NVIDIA RTX 2000 Ada Generation (16380 MB), 14634 MB allocated

Quantization:
  GPU experts:    INT4 (Marlin)
  CPU experts:    INT4
  Attention:      INT8
  Shared expert:  INT8
  Dense MLP:      INT8
  LM head:        INT8
  KV cache:       FP8 E4M3

Strategy:
  Expert divisor: -3 (hot_cached_static)
  Prefill threshold: 1
  Mode:           gpu_decode (hot_cached_static)
  Extended HCS:   3 GPUs (1 primary + 2 aux), 20825/24576 experts on GPU (84.7%), 3751 cold

Prefill (10000 tokens, 3 runs):
  Run 1:  533.2 tok/s, TTFT=18.75s
  Run 2:  282.1 tok/s, TTFT=35.45s
  Run 3:  531.5 tok/s, TTFT=18.81s
  Average: 448.9 tok/s, TTFT=24.34s

Decode (64 tokens, 3 runs):
  Run 1:  9.05 tok/s (110.6ms/tok)
  Run 2:  9.12 tok/s (109.7ms/tok)
  Run 3:  9.07 tok/s (110.3ms/tok)
  Average: 9.08 tok/s (110.2ms/tok)

Verification:
  Generated output (64 tokens):
    **The Recursive Ladder**

    A function stands, both firm and tall,
    Yet whispers low: *"I call myself."*
    No loop of iron, no endless spin—
    Just self-reflection, deep within.

    It asks: *"Shall I descend or stay?
    A base case first
================================================================
